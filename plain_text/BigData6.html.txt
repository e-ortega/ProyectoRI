 press x to close   Pasar al contenido principal                             Contacto      CatalÃ  EspaÃ±ol  English                        Soluciones Empresas Soluciones para empresas  Ãreas especializaciÃ³n  Colaboradores  FormaciÃ³n   Proyectos  Herramientas CitScale   Programa Talent  Laboratorio Docente  Blog  inLab FIB Conoce inLab FIB  Equipo                Formulario de bÃºsqueda   Search this site                            Â¿QuÃ© herramientas necesitas para iniciarte en Big Data?                   Enviado por Juan Salmeron en Vie, 15/01/2016 - 13:54               IntroducciÃ³n  Desde la creaciÃ³n de Internet y cada vez mÃ¡s, se ha contado con datos generados por infinidad de aplicaciones y que tenemos a nuestro alcance esperando a ser usados. AntaÃ±o, tener y consumir todo este ingente material se podÃ­a volver una tarea prÃ¡cticamente imposible o inviable tanto econÃ³mica como tecnolÃ³gicamente. Esto limitaba el uso de la informaciÃ³n a aquella que era mÃ¡s relevante y punto. Vamos a ver en que herramientas podemos iniciarnos para solventar este handicap y sacar el mÃ¡ximo valor a nuestros datos . Herramientas open-source y que cualquiera tiene a su disposiciÃ³n. El ecosistema Hadoop .  Hardware  Hoy en dÃ­a tenemos a nuestro alcance el poder consumir esta informaciÃ³n sin necesidad de contar con un carÃ­simo supercomputador en nuestro trastero. Podemos llegar a hacer pruebas con sistemas de 8-16GB de RAM y unos cuantos GB de disco duro. En un entorno de producciÃ³n se usa " comodity hardware ", es decir, mÃ¡quinas que no necesitan tener un alto grado de fiabilidad y sofisticaciÃ³n (sistemas RAID, discos duros enterprise, componentes redundantes, etc). MÃ¡quinas de este tipo son mucho mÃ¡s baratas y si se rompen, ponemos otras y ya estÃ¡. El peso de la fiabilidad recae sobre el software. Tendremos, esto sÃ­, que dimensionar las caracterÃ­sticas segÃºn los requerimientos de nuestro escenario. Una configuraciÃ³n tÃ­pica de un nodo de un entorno productivo podrÃ­a ser una mÃ¡quina con 8-24 cores, 32-256GB de RAM y unos 8-12 discos HDD.  Para introducirnos en el uso de este software, no es necesario tener un cluster de mÃ¡quinas potentes, podremos ponernos a trastear con un Â¿ Cubieboard cluster :)?, unas simples VM's en nuestro PC o con alguna VM como la Cloudera QuickStart VM con todo ya montado, que sirve justamente para aprender.  MotivaciÃ³n   Antes de nada, dejemos clara una cosa y no nos engaÃ±emos, Big Data es para tratar con grandes volÃºmenes de datos. Big Data aparece cuando decidimos dejar de quedarnos con lo que era mÃ¡s relevante y pasar a quedarnos con TODO . Toda informaciÃ³n sirve en algÃºn momento y nos puede permitir ver cosas que con solamente "lo importante" no podemos llegar a ver y que en muchas ocasiones nos puede aportar mucho valor .  Como decÃ­amos, hoy en dÃ­a se ha vuelto factible tener mucha informaciÃ³n y ser capaz de consumirla, pero tambiÃ©n hay que entender cuando tiene sentido usar herramientas Big Data y cuando no. Si tenemos un volumen alto de datos y tenemos que lidiar con gigas y gigas de datos (o alguna magnitud mayor) estarÃ¡ bien usar estas herramientas. De no ser asÃ­, tambiÃ©n podemos hacer uso de todo esto, pero matar moscas a caÃ±onazos quizÃ¡s no es tan adecuado, aunque sea la moda.  La Base  Hablar de Big Data es hablar de Hadoop y todo lo que lo rodea. Vamos a ver quÃ© es Hadoop. Hadoop se sustenta en la forma en la que almacena y accede a los datos. Hadoop estÃ¡ formado por HDFS y MapReduce. La combinaciÃ³n de estos dos permite que los datos estÃ©n replicados y distribuidos por N nodos beneficiando la capacidad de acceso a grandes volÃºmenes. Cuando queremos ejecutar alguna operaciÃ³n sobre estos datos distribuidos, Hadoop se encarga de procesar cada porciÃ³n de los datos en el nodo que los contiene . De esta forma se aprovecha la localidad de tener los datos cerca de donde se van a procesar y permite escalar de forma casi lineal. Si queremos crecer en capacidad, aÃ±adimos mÃ¡s nodos y listo. Del almacenamiento se encarga HDFS y del procesamiento MapReduce.   Complementos BÃ¡sicos  Con HDFS y MapReduce tenemos la capacidad bÃ¡sica de almacenar datos en crudo y realizar procesos en paralelo abstrayÃ©ndonos de la complejidad de este tipo de computaciÃ³n. Ahora bien, podemos utilizar otras herramientas encima de Hadoop que nos potencian estas capacidades. Todo dependerÃ¡ de nuestras necesidades. Estos son los dos mÃ¡s tÃ­picos:  HDFS  Es el filesystem en el que se basa Hadoop. Este sistema de ficheros se basa en una arquitectura master-slave donde los MasterNodes coordinan a los DataNodes, que son aquellos donde se guarda la informaciÃ³n. Los datos en HDFS se distribuyen por los diferentes DataNodes en particiones del fichero original, asegurando que cada una de estas particiones estÃ¡ replicada en un nivel de replicaciÃ³n definido. Por tanto, no necesitamos disponer de sistemas con RAID, si un disco se estropea, HDFS automÃ¡ticamente replica muy rÃ¡pidamente todas las particiones que estaban en este disco en todo el resto de DataNodes.  â€‹  YARN - MapReduce  YARN es la evoluciÃ³n de MapReduce para Hadoop 2.0. La funciÃ³n de YARN en Hadoop es la de proporcionar un entorno que gestione los recursos para realizar trabajos de computaciÃ³n. YARN se ocupa de distribuir el trabajo a hacer teniendo en cuenta donde estÃ¡n los datos a procesar, ademÃ¡s de gestionar las propias ejecuciones de los programas. Con YARN podemos separar el sistema de ficheros del sistema de ejecuciÃ³n. Por tanto podemos usar HDFS sin pasar por YARN o bien usar este gestor de recursos para correr aplicaciones (Como es el caso de la mayorÃ­a de herramientas que trabajan en Hadoop).  HBase  Se puede considerar "la base de datos" de Hadoop. Basado en BigTable de Google, proporciona la capacidad de crear tablas con millones de entradas y permite hacer accesos de lectura/escritura rÃ¡pida y consistentemente. TambiÃ©n es versionada y no-relacional, por lo que ofrece flexibilidad. AdemÃ¡s es fÃ¡cilmente conectable, ya sea a travÃ©s de su API java o mediante web services. Podemos usar HBase para dar forma a nuestros datos.  Hive  Es una mezcla entre MapReduce y HBase. Permite estructurar los datos en tablas y vistas y nos permite realizar todo tipo de consultas usando su lenguaje de querying tipo SQL, el HiveQL. Para aquellos casos en los que no podamos obtener lo que queremos con HiveQL, podemos conectar Hive con nuestros propios mappers y reducers, ya que Hive es muy flexible en cuanto a conectividad y se le puede conectar cualquier cosa.  Spark  Es el rey que ha venido a conquistarlos a todos. Mientras que MapReduce realiza sus procesos sobre disco, Spark carga en memoria los datos y realiza operaciones entre datasets intermedios llamados RDDs. Esto hace que su rendimiento sea brutal. AdemÃ¡s es bastante sencillo de utilizar y tiene soporte tanto para Scala como para Java y Python, contando con librerÃ­as para realizar procesamiento en micro-batch (STREAMING), machine learning (MLLIB), grafos (GRAPH) y SQL (SPARKSQL).  Otros Complementos  AdemÃ¡s de la suite tÃ­pica que hemos visto, hay muchos otros productos que funcionan junto con Hadoop y permiten nuevas funcionalidades. SegÃºn el tipo de aplicaciÃ³n usaremos unas u otras. TambiÃ©n existen variantes de estas herramientas adaptadas a usos especÃ­ficos , como SparkOnHBase , Spork , RHadoop y otros muchos mÃ¡s. AquÃ­ tenÃ©is una minÃºscula clasificaciÃ³n: Data Engineering: Spark , Hive , Pig Data Discovery & Analytics: Spark , Impala , Solr Data Integration & Storage: HBase , Kudu , HDFS Unified Data Services: Yarn , Sentry , Hue , Oozie Data Ingestion: Sqoop , Flume , Kafka  Hands o n  Hemos visto solo una pincelada de las herramientas que se usan para BigData. Dominarlas es cuestiÃ³n de horas y de prÃ¡ctica. Para practicar, podÃ©is hacer uso de la VM que anteriormente os comentaba para poner en practica alguno de los muchos cursos y tutoriales que se pueden encontrar on-line, como este curso gratuito , que te enseÃ±arÃ¡n desde hacer el clÃ¡sico WordCount hasta realizar tareas de Analytics mÃ¡s complejas. TambiÃ©n recomiendo encarecidamente echar mano de los clÃ¡sicos libros O'Reilly especÃ­ficos de cada tecnologÃ­a.              Proyectos relacionados         RA Board (MonitorizaciÃ³n de Recursos para el Aprendizaje)     Model@     Barcelona Virtual Mobility Lab     Suport@ (Soporte a la Docencia)     Barcelona, Smart Shuttle Pilot     Un termÃ³metro emocional para la docencia     UOC Ãndex. Usando Learning Analytics para medir el eLearning     Big Data Analytics Lab     Learning analytics     Learning analytics - PILARES        ArtÃ­culos relacionados         Â¿QuÃ© es la VisualizaciÃ³n de datos - DataViz?     Big Data aplicado al deporte     Â¿QuÃ© es un Data Scientist?                    SÃ­guenos en                  inLab FIB incorpora esCert             inLab es miembro de                    Este web usa cookies propias para ofrecer una mejor experiencia y servicio. Al continuar con la navegaciÃ³n entendemos que aceptas nuestra polÃ­tica de cookies . inLab FIB | +34 93 401 69 41 | c/ Jordi Girona, 1-3. Edificio B6. 08034 Barcelona | inlab fib.upc.edu | Intranet | Sobre esta web           