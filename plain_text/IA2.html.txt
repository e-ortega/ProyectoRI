Â­    Pasar al contenido principal      .         Secciones + Biomedicina  ComputaciÃ³n  EnergÃ­a  MÃ³vil  Negocios  RobÃ³tica  10 TecnologÃ­as Emergentes  Las 50 Empresas mÃ¡s Inteligentes  OpiniÃ³n  Habla el mercado  Innovadores a Fondo   Actualidad  Innovadores  MÃ¡s + Eventos     Formulario de bÃºsqueda            Menu    Suscribirse Arrow  Buscar   Secciones + Biomedicina ComputaciÃ³n EnergÃ­a MÃ³vil Negocios RobÃ³tica 10 TecnologÃ­as Emergentes Las 50 Empresas mÃ¡s Inteligentes OpiniÃ³n Habla el mercado Innovadores a Fondo Actualidad Innovadores MÃ¡s + Eventos      Escriba las palabras clave.                                                       ComputaciÃ³n   El secreto mÃ¡s oscuro de la inteligencia artificial: Â¿por quÃ© hace lo que hace?  1    Por muy buenas que sean las 
predicciones del aprendizaje profundo, nadie sabe cÃ³mo llega a sus 
conclusiones. Este hecho estÃ¡ empezando a generar un problema de 
confianza y las herramientas para resolverlo no estÃ¡n ayudando 
demasiado. Pero tal vez asÃ­ es la propia naturaleza de la inteligencia                                        por Will Knight | traducido por Teresa Woods  17 Abril, 2017                       El aÃ±o pasado, un extraÃ±o coche autÃ³nomo fue liberado en las 
tranquilas calles de Monmouth County (EEUU). El prototipo, que fue 
desarrollado por investigadores del fabricante de chips Nvidia, no tenÃ­a
 un aspecto muy distinto al resto de coches autÃ³nomos y, sin 
embargo, no se parecÃ­a a nada de lo que han hecho Google, Tesla o 
General Motors. AdemÃ¡s, era una prueba del poder cada vez mayor de la 
inteligencia artificial (IA). El coche no seguÃ­a las indicaciones de un 
ingeniero o un programador. En su lugar, dependÃ­a de un algoritmo que se habÃ­a enseÃ±ado a conducir a sÃ­ mismo al observar a un conductor humano.  Hacer que el coche aprendiera a conducir bajo este enfoque fue una 
hazaÃ±a impresionante. Pero tambiÃ©n resulta algo desconcertante, ya que 
no estÃ¡ claro cuÃ¡l es su mecanismo para la toma de deciciones. Los datos
 que recogen los sensores del vehÃ­culo entran directamente en una enorme
 red de neuronas artificiales que los procesan. A partir de ellos, el 
ordejador decide quÃ© harÃ¡ y envia las Ã³rdenes correspondientes al 
volante, los frenos y otros sistemas para que las lleven a cabo. El 
resultado parece igualar el comportamiento que se esperarÃ­a de un 
conductor humano. Pero, Â¿y si algÃºn dÃ­a hiciera algo inesperado como 
chocar contra un Ã¡rbol, o quedarse inmÃ³vil ante un semÃ¡foro en verde? 
Tal y como estÃ¡n las cosas ahora mismo, podrÃ­a resultar difÃ­cil averiguar por quÃ© hace lo que hace .
 El sistema es tan complejo que incluso los ingenieros que lo diseÃ±aron 
podrÃ­an ser incapaces de aislar la razÃ³n de cualquier acciÃ³n 
determinada. Y no se le puede preguntar: no existe ninguna manera obvia 
de diseÃ±ar un sistema para que pueda explicar el por quÃ© de lo que ha 
hecho (ver Se acabÃ³ la confianza ciega, la inteligencia artificial debe explicar cÃ³mo funciona ).  La misteriosa mente de este vehÃ­culo seÃ±ala un problema cada vez mÃ¡s 
palpable de la inteligencia artificial. La tecnologÃ­a de IA del coche, 
conocida como aprendizaje profundo ha tenido mucho Ã©xito resolviendo problemas en los Ãºltimos aÃ±os, y cada
 vez se usa mÃ¡s en labores como generar subtÃ­tulos, reconocer la voz y 
traducir idiomas. Estas mismas tÃ©cnicas podrÃ­an llegar a ser capaces de diagnosticar enfermedades mortales, tomar decisiones bursÃ¡tiles multimillonarias y transformar industrias al completo.  Pero esto no sucederÃ¡  (o no deberÃ­a) a menos que consigamos que las tÃ©cnicas como el aprendizaje profundo resulten mÃ¡s comprensibles para sus creadores y rindan cuentas ante los usuarios .
 En caso contrario, resultarÃ¡ difÃ­cil predecir cuÃ¡ndo se podrÃ­an 
producir fallos, los cuales son inevitables. Este es uno de los motivos 
por los que el coche de Nvidia aÃºn es experimental.  Ya se estÃ¡n empleando modelos matemÃ¡ticos para ayudar a determinar quiÃ©n recibe la libertad condicional , quiÃ©n es apto para obtener prÃ©stamos y quiÃ©n es contratado para ocupar un puesto vacante .
 Si se pudiese acceder a estos modelos matemÃ¡ticos, serÃ­a posible 
entender su razonamiento. Pero los bancos, el ejÃ©rcito y los empleadores
 estÃ¡n centrÃ¡ndose en enfoques de aprendizaje automÃ¡tico aÃºn mÃ¡s 
complejos que podrÃ­an resultar totalmente inescrutables. El aprendizaje 
profundo, el mÃ¡s comÃºn de estos enfoques, representa una manera 
fundamentalmente distinta de programar los ordenadores. " Ya es un problema relevante, y lo va a ser mucho mÃ¡s en el futuro ",
 afirma el profesor del Instituto TecnolÃ³gico de Massachusetts (MIT, 
EEUU) Tommi Jaakkola, que trabaja en aplicaciones del aprendizaje 
automÃ¡tico. El experto aÃ±ade: "Tanto si es una decisiÃ³n de inversiÃ³n, 
mÃ©dica o militar, nadie quiere tener que depender de un mÃ©todo de 'caja negra' ".  Ya hay argumentos que defienden que la capacidad de interrogar a un 
sistema de inteligencia artificial sobre cÃ³mo llegÃ³ a sus conclusiones 
representa un derecho legal bÃ¡sico . A partir del verano
 de 2018, la UniÃ³n Europea podrÃ­a obligar a las empresas a que ofrezcan 
una respuesta a sus ususarios sobre las decisiones a las que llegan los 
sistemas automatizados. Esto podrÃ­a resultar imposible, incluso para sistemas aparentemente sencillos , como las apps y pÃ¡ginas web que emplean el aprendizaje profundo para mostrar anuncios
 y recomendar canciones. Los ordenadores que ejecutan esos servicios se 
han autoprogramado y ni siquiera sabemos cÃ³mo. Incluso los 
ingenieros que desarrollan estas apps son incapaces de explicar totalmente su comportamiento.  Esto suscita algunas preguntas inquietantes. A medida que la tecnologÃ­a avance podrÃ­amos cruzar un umbral a partir del cual utilizar la IA requiera un salto de fe .
 Claro que los humanos no siempre podemos explicar realmente nuestros 
procesos cognitivos tampoco, pero encontramos maneras de confiar en y 
juzgar a la gente de forma intuitiva. Â¿TambiÃ©n serÃ¡ posible eso con 
mÃ¡quinas que piensan y toman decisiones de manera distinta a los 
humanos? Nunca habÃ­amos desarrollado mÃ¡quinas que trabajan sin que sus creadores entiendan cÃ³mo .
 Â¿Esperamos llevarnos y comunicarnos bien con mÃ¡quinas inteligentes que 
podrÃ­an ser impredecibles e inescrutables? Estas preguntas me empujaron a
 emprender un viaje hasta la frontera de las investigaciones sobre 
algoritmos de inteligencia artificial, desde Google hasta Apple, pasando
 por muchos sitios entre medias, incluida una reuniÃ³n con uno de los 
grandes filÃ³sofos de nuestro tiempo.   Foto: El artista Adam Ferriss creÃ³ esta
 imagen, y la de mÃ¡s abajo, con el uso de Google Deep Dream, un programa
 que ajusta una imagen para simular las capacidades de reconocimiento de
 patrones de una red neuronal profunda. Las imÃ¡genes fueron producidas 
por una capa intermedia de la red neuronal. CrÃ©dito: Adam Ferriss.  En 2015, un grupo de investigadores del Hospital Monte SinaÃ­ de Nueva
 York (EEUU) quiso aplicar el aprendizaje profundo a la vasta base de 
datos de historiales de pacientes del hospital. Este conjunto de datos 
incluye cientos de variables, procedentes de los resultados de pruebas, 
consultas mÃ©dicas y mucho mÃ¡s. El programa resultante, que los 
investigadores llamaron Deep Patient, fue entrenado con datos de 
alrededor de 700.000 individuos, y cuando fue probado con historiales 
nuevos, resultÃ³ ser increÃ­blemente bueno a la hora de precedir enfermedades .
 Sin ninguna formaciÃ³n por parte de expertos, Deep Patient habÃ­a 
descubierto patrones ocultos dentro de los datos del hospital que 
parecÃ­an indicar cuÃ¡ndo la gente estaba a punto de desarrollar un amplio
 abanico de trastornos, incluido el cÃ¡ncer de hÃ­gado. Hay muchos mÃ©todos
 para predecir enfermedades a partir del historial mÃ©dico de un paciente
 que rinden "bastante bien", segÃºn el lÃ­der del equipo, Joel Dudley. 
Pero, aÃ±ade: "Esto simplemente era mucho mejor ".  "Podemos elaborar estos modelos, pero no sabemos cÃ³mo funcionan".   Al mismo tiempo, Deep Patient es algo desconcertante. Parece 
anticipar el comienzo de trastornos psiquiÃ¡tricos como la esquizofrenia 
bastante bien. Pero dado que es un trastorno que a los mÃ©dicos les cuesta mucho predecir, Dudley no entendÃ­a los resultados de su algoritmo. Y sigue sin hacerlo. La nueva 
herramienta da pistas sobre cÃ³mo lo hace. Si algo como Deep Patient 
puede ayudar a los mÃ©dicos, lo ideal serÃ­a que compartiera el 
razonamiento que le ha llevado hasta su predicciÃ³n para asegurarse de 
que es precisa y poder justificar, por ejemplo, un cambio en la 
medicaciÃ³n que recibe un paciente. Con una sonrisa algo triste, Dudley 
afirma: "Podemos desarrollar estos modelos, pero no sabemos cÃ³mo 
funcionan".  La inteligencia artificial no siempre ha sido asÃ­. Desde un 
principio, ha habido dos escuelas de pensamiento respecto a lo 
entendible, o explicable, que deberÃ­a ser. Muchos creÃ­an que tenÃ­a 
sentido desarrollar mÃ¡quinas que razonaran de acuerdo a reglas y lÃ³gica,
 lo que volverÃ­a transparente su funcionamiento interno para cualquiera 
que quisiera examinar el cÃ³digo. Otros sentÃ­an que la inteligencia 
avanzarÃ­a mÃ¡s si las mÃ¡quinas se inspiraran en la biologÃ­a, y 
aprendieran mediante la observaciÃ³n y la experiencia. Esto significaba 
invertir la programaciÃ³n informÃ¡tica. En lugar de que un programador 
escribiera los comandos para resolver un problema, el programa genera su
 propio algoritmo en base a datos de ejemplo y el resultado deseado. Las
 tÃ©cnicas de aprendizaje automÃ¡tico que evolucionaron para convertirse 
en los sistemas de IA mÃ¡s potentes de la actualidad siguieron el segundo camino: bÃ¡sicamente, la mÃ¡quina se autoprograma.  Al principio, este enfoque no tenÃ­a demasiadas aplicaciones, y 
durante las dÃ©cadas de 1960 y 1970 seguÃ­a ocupando la periferia del 
campo. Entonces la informatizaciÃ³n de muchas industrias y la llegada del big data renovaron el interÃ©s. Eso inspirÃ³ el desarrollo de tÃ©cnicas de aprendizaje automÃ¡tico mÃ¡s 
potentes, especialmente nuevas versiones de una tÃ©cnica conocida como 
red neuronal artificial. Para finales de la dÃ©cada de 1990, las redes 
neuronales podÃ­an digitalizar automÃ¡ticamente caracteres escritos a mano .  Pero no fue hasta principios de esta dÃ©cada, tras varios ingeniosos 
ajustes y refinamientos, cuando las redes neuronales muy grandes, o 
"profundas", empezaron a ofrecer drÃ¡sticas mejoras en la percepciÃ³n 
automatizada. El aprendizaje profundo es responsable de la explosiÃ³n actual de la IA .
 Ha dotado los ordenadores de poderes extraordinarios, como la capacidad
 de reconocer las palabras habladas casi igual de bien que cualquier 
persona, algo demasiado complejo para ser codificado a mano. El 
aprendizaje profundo ha transformado la visiÃ³n de mÃ¡quinas y mejorado 
muchÃ­simo la traducciÃ³n automatizada. Ya contribuye en la toma de todo 
tipo de decisiones claves en la medicina, las finanzas, la fabricaciÃ³n y
 mucho mÃ¡s.   El funcionamiento de cualquier tecnologÃ­a de aprendizaje automÃ¡tico 
es inherentemente mÃ¡s opaco que un sistema codificado a mano, incluso 
para los informÃ¡ticos. Esto no quiere decir que todas las futuras 
tÃ©cnicas de IA vayan a resultar igual de imposibles de entender. Pero, 
por su naturaleza, el aprendizaje profundo es una caja negra 
especialmente oscura.  Simplemente no es posible adentrarse en las entraÃ±as de una red neuronal profunda para comprobar cÃ³mo funciona. Su razonamiento estÃ¡ arraigado en el comportamiento de miles de neuronas simuladas ,
 dispuestas en docenas o incluso cientos de capas intricadamente 
interconectadas. Las neuronas de la primera capa reciben informaciones, 
como la intensidad de un pÃ­xel dentro de una imagen, y despuÃ©s realizan 
un cÃ¡lculo antes de emitir una nueva seÃ±al. Estas seÃ±ales alimentan las 
neuronas de la prÃ³xima capa de una compleja red, y asÃ­ sucesivamente 
hasta generar un resultado final. AdemÃ¡s, hay un proceso conocido como 
propagaciÃ³n hacia atrÃ¡s que ajusta los cÃ¡lculos de neuronas individuales
 para que la red aprenda a producir un resultado deseado.  Las mÃºltiples capas de una red profunda la habilitan para reconocer cosas a muchos niveles distintos de abstracciÃ³n. En un sistema diseÃ±ado para reconocer perros, por ejemplo, las primeras
 capas reconocen elementos muy bÃ¡sicos como contornos y colores; las 
siguientes capas reconocen cosas mÃ¡s complejas como el pelo o los ojos; y
 las capas superiores objetos al completo, como un perro. El mismo 
enfoque puede aplicarse a otras Ã¡reas que permiten que las mÃ¡quinas se 
enseÃ±en a sÃ­ mismas: los sonidos que componen las palabras dentro del 
habla, las letras y palabras que generan frases dentro de un texto o los
 movimientos de volante requeridos para la conducciÃ³n.  "Simplemente podrÃ­a ser una parte de la propia 
naturaleza de la inteligencia el hecho de que sÃ³lo una fracciÃ³n estÃ¡ 
sujeta a explicaciones racionales. Parte de ella es simplemente 
instintiva".    Se han empleado ingeniosas estrategias para intentar captar y 
explicar en mayor detalle lo que sucede dentro de estos sistemas. En 
2015, unos investigadores de Google modificaron un algoritmo de 
reconocimiento de imÃ¡genes por aprendizaje profundo para que en lugar de
 divisar objetos dentro de las fotos los generara o modificara. Al ejecutar el algoritmo al revÃ©s, descubrieron las caracterÃ­sticas que el programa emplea para reconocer un pÃ¡jaro o un edificio. Las imÃ¡genes resultantes ,
 producidas por un proyecto conocido como Deep Dream, mostraban animales
 grotescos con aspecto alienÃ­gena emergiendo de nubes y plantas, y 
pagodas alucinatorias que poblaban bosques y cordilleras. Las imÃ¡genes 
demostraron que el aprendizaje profundo no tiene por quÃ© ser del todo 
inescrutable. Revelaron que los algoritmos se centran en caracterÃ­sticas
 visuales familiares como el pico o las plumas de un pÃ¡jaro. Pero las 
imÃ¡genes tambiÃ©n ofrecieron pistas de lo mucho que difiere de la 
percepciÃ³n humana, ya que podrÃ­an tomar decisiones basadas en elementos 
que nosotros sabrÃ­amos ignorar. Los investigadores de Google seÃ±alaron 
que cuando su algoritmo generaba imÃ¡genes de una mancuerna, tambiÃ©n 
generaba un brazo humano sujetÃ¡ndola. La mÃ¡quina habÃ­a concluido que el brazo formaba parte del objeto.  Y la neurociencia y la ciencia cognitiva han logrado aÃºn mÃ¡s avances.
 Un equipo liderado por el profesor adjunto de la Universidad de Wyoming
 (EEUU) Jeff Clune ha empleado el equivalente de IA de las ilusiones 
Ã³pticas para probar redes neuronales profundas.  En 2015, el grupo de Clune demostrÃ³ cÃ³mo ciertas imÃ¡genes podÃ­an engaÃ±ar a una red para percibir cosas que no estaban allÃ­ ,
 porque las imÃ¡genes se aprovechan de los patrones de nivel bajo que el 
sistema rastrea. Uno de los colaboradores de Clune, Jason Yosinski, 
tambiÃ©n desarrollÃ³ una herramienta que actÃºa como sonda cerebral. Se 
dirige a cualquier neurona de la red y busca la imagen que mÃ¡s la 
active. Las imÃ¡genes devueltas son abstractas (imagÃ­nese una versiÃ³n 
impresionista de un flamenco o autobÃºs escolar), lo que, de nuevo, 
revela la naturaleza misteriosa de las capacidades de percepciÃ³n de la 
mÃ¡quina.    Foto: Esta temprana red neuronal artificial, del Laboratorio AeronÃ¡utica  de
 la Universidad de Cornell en BÃºfalo, Nueva York, de alrededor de 1960, 
procesaba informaciones procedentes de sensores de luz. CrÃ©dito:  Frederic Lewis (Getty Images).     Foto: Ferriss se sintiÃ³ inspirado a 
pasar la red neuronal artificial de la Universidad de Cornell por Deep 
Dream, generando asÃ­ esta imagen y la de mÃ¡s abajo. CrÃ©dito: Adam Ferriss.  Pero no podemos conformarnos con una visiÃ³n aproximada de cÃ³mo funciona la inteligencia artificial, y parece que no hay 
soluciones fÃ¡ciles. Lo que resulta crucial para reconocer patrones y 
tomar decisiones complejas de nivel superior es la interacciÃ³n de 
cÃ¡lculos dentro de una red neuronal profunda. Pero esos cÃ¡lculos son una
 ciÃ©naga de funciones y variables matemÃ¡ticas. "Si tuvieras una red neuronal muy pequeÃ±a, tal vez podrÃ­as entenderla .
 Pero en cuanto se vuelve muy grande y presenta miles de unidades por 
capa y tal vez cientos de capas, entonces se vuelve bastante 
incomprensible", dice Jaakkola.  En el despacho de al lado al de Jaakkola trabaja la profesora del MIT
 Regina Barzilay, que estÃ¡ empeÃ±ada en aplicar el aprendizaje automÃ¡tico
 a la medicina. Fue diagnosticada con cÃ¡ncer de mama hace un par de 
aÃ±os, a la edad de 43. El diagnÃ³stico le resultÃ³ impactante, pero 
Barzilay tambiÃ©n estaba consternada por el hecho de que los 
mÃ©todos estadÃ­sticos mÃ¡s modernos y el aprendizaje automÃ¡tico no 
estuvieran siendo empleados para apoyar las investigaciones oncolÃ³gicas 
ni para guiar los tratamientos de los pacientes. Asegura que la
 IA tiene un enorme potencial para revolucionar la medicina, pero 
alcanzar ese potencial significarÃ¡ ir mÃ¡s allÃ¡ de los historiales 
mÃ©dicos. Le gustarÃ­a que se emplearan muchos mÃ¡s datos, los cuales 
asegura que estÃ¡n infrautilizados: "datos de imÃ¡genes, datos 
patolÃ³gicos, todas estas informaciones", aclara.  Â¿Esperamos llevarnos y comunicarnos bien con mÃ¡quinas inteligentes que podrÃ­an ser impredecibles e inescrutables?   DespuÃ©s de finalizar su tratamiento el aÃ±o pasado, Barzilay y sus 
alumnos empezaron a colaborar con mÃ©dicos del Hospital General de 
Massachusetts (EEUU) para desarrollar un sistema capaz de minar los 
informes de patologÃ­as para identificar pacientes con determinadas 
caracterÃ­sticas que los investigadores tal vez querrÃ­an estudiar. Sin 
embargo, Barzilay entendÃ­a que el sistema tendrÃ­a que explicar su 
razonamiento. AsÃ­ que, junto con Jaakkola y un alumno, aÃ±adiÃ³ un paso: 
el sistema extrae y seÃ±ala las partes de un texto que mÃ¡s representan el patrÃ³n que ha descubierto .
 Barzilay y sus alumnos tambiÃ©n estÃ¡n desarrollando un algoritmo de 
aprendizaje profundo capaz de identificar las primeras seÃ±ales del 
cÃ¡ncer de mama en imÃ¡genes de mamografÃ­a y quieren dotarlo de la 
capacidad de explicar su razonamiento. "Realmente necesitas tener un 
bucle de colaboraciÃ³n entre la mÃ¡quina y el humano", explica Barzilay.  El EjÃ©rcito de Estados Unidos estÃ¡ invirtiendo miles de millones de 
euros en proyectos que emplearÃ¡n el aprendizaje automÃ¡tico para pilotar 
vehÃ­culos y aeronaves, identificar objetivos y ayudar a los analistas a 
procesar enormes cantidades de datos de inteligencia. AquÃ­, mÃ¡s que en 
ningÃºn otro sitio, incluso mÃ¡s que en la medicina, existe poco margen 
para los misterios algorÃ­tmicos y el Departamento de Defensa de EEUU considera que la incapacidad de explicarse es un obstÃ¡culo clave.  El director de programa de la Agencia de Proyectos de InvestigaciÃ³n 
Avanzados de EEUU (DARPA, por sus siglas en inglÃ©s) David Gunning estÃ¡ 
supervisando el muy bien nombrado Programa de Inteligencia Artificial 
Explicable. Este veterano de la agencia, de pelo gris y que supervisÃ³ el
 proyecto de DARPA que finalmente dio paso a la creaciÃ³n de Siri, dice 
que la automatizaciÃ³n se estÃ¡ colando en innumerables Ã¡reas del 
ejÃ©rcito. Analistas de inteligencia estÃ¡n probando el aprendizaje 
automÃ¡tico para identificar patrones en vastas cantidades de datos de 
vigilancia. Muchos vehÃ­culos autÃ³nomos terrestres y aÃ©reos estÃ¡n siendo 
desarrollados y probados. Pero los soldados probablemente no se sentirÃ¡n cÃ³modos dentro de un tanque robÃ³tico del que no saben por quÃ© hace lo que hace ,
 y los analistas se mostrarÃ¡n reacios a actuar en funciÃ³n 
de unas recomendaciones generadas sin algÃºn tipo de razonamiento. 
"A menudo estos sistemas generan muchas falsas alarmas, por lo que un 
analista de inteligencia realmente necesita una ayuda extra para 
entender por quÃ© se ha hecho una recomendaciÃ³n", seÃ±ala Gunning.  El pasado mes de marzo, DARPA escogiÃ³ 13 proyectos de la academia y 
la industria para financiarlos bajo el programa de Gunning. Algunos de 
ellos podrÃ­an basarse en el trabajo liderado por el profesor de la 
Universidad de Washington (EEUU) Carlos Guestrin. Junto a sus 
compaÃ±eros, ha desarrollado una manera de que los sistemas de aprendizaje automÃ¡tico expliquen sus resultados .
 En esencia, bajo este mÃ©todo un ordenador encuentra automÃ¡ticamente 
unos pocos ejemplos dentro de un conjunto de datos y proporciona una 
corta explicaciÃ³n para ellos. Un sistema diseÃ±ado para clasificar un 
mensaje de correo electrÃ³nico como procedente de un terrorista, por 
ejemplo, podrÃ­a emplear millones de mensajes en su entrenamiento y 
proceso de toma de decisiones. Pero mediante el enfoque del equipo de 
Washington, podrÃ­a seÃ±alar determinadas palabras claves encontradas 
dentro de un mensaje. El grupo de Guestrin tambiÃ©n ha elaborado 
herramientas para que los sistemas de reconocimiento de imÃ¡genes den 
pistas sobre su razonamiento al seÃ±alar las partes de una imagen que 
resultaron mÃ¡s importantes.     Pero estos enfoques tienen una pega: las explicaciones siempre 
estarÃ¡n simplificadas, lo que significa que algunas informaciones 
vitales podrÃ­an perderse por el camino. Guestrin detalla: "No hemos 
logrado el sueÃ±o al completo, que es donde la IA mantiene una 
conversaciÃ³n contigo y es capaz de explicarse. AÃºn estamos muy lejos de disponer de una IA verdaderamente interpretable ".  Y no es necesario que se trate de temas cruciales, como el 
diagnÃ³stico de un cÃ¡ncer, para que esto resulte un problema. Conocer el 
razonamiento de la IA tambiÃ©n serÃ¡ imprescindible si la tecnologÃ­a 
aspira a formar parte de la vida diaria de las personas. El lÃ­der del 
equipo de Siri de Apple, Tom Gruber, dice que el carÃ¡cter explicable es 
una consideraciÃ³n clave para su equipo en sus esfuerzos por hacerla mÃ¡s 
inteligente y hÃ¡bil. Gruber no quiso hablar de planes especÃ­ficos para 
el futuro de Siri, pero resulta fÃ¡cil imaginar que al recibir una 
recomendaciÃ³n de un restaurante, se quiera conocer el razonamiento 
subyacente. El director de investigaciones de IA de Apple y profesor de 
la Universidad de Carnegie Mellon (EEUU), Ruslan Salakhutdinov, 
considera que la capacidad de explicarse es el nÃºcleo de la relaciÃ³n en evoluciÃ³n entre los humanos y las mÃ¡quinas inteligentes. "Va a generar confianza", afirma.  Al igual que muchos aspectos del comportamiento humano resultan 
imposibles de explicar en detalle, tal vez no serÃ¡ posible que la 
inteligencia artificial llegue a explicar todo lo que hace. "Incluso si 
alguien te puede dar una explicaciÃ³n razonable [de sus acciones], 
probablemente estarÃ¡ incompleta, y lo mismo podrÃ­a aplicarse a la 
inteligencia artificial", sugiere Clune. El experto aÃ±ade: "Simplemente podrÃ­a ser una parte de la propia naturaleza de la inteligencia el
 hecho de que sÃ³lo una fracciÃ³n estÃ¡ sujeta a explicaciones racionales. 
Parte de ella es simplemente instintiva, o subconsciente o 
inescrutable".  En tal caso, tal vez llegue un momento en el que debamos decidir si 
confiamos ciegamente en la IA o desechamos su uso. Igualmente, esa misma
 decisiÃ³n tendrÃ¡ que incorporar inteligencia social. Al igual que la 
sociedad se construye sobre una base de comportamientos aceptables, necesitaremos diseÃ±ar los sistemas de IA para respetar y encajar con nuestras normas sociales .
 Si vamos a crear tanques robÃ³ticos y otras mÃ¡quinas de matar, es 
importante que su toma de decisiones concuerde con nuestros juicios 
Ã©ticos.  Para explorar estos conceptos metafÃ­sicos, acudÃ­ a la Universidad de 
Tufts (EEUU) para reunirme con el filÃ³sofo de renombre y cientÃ­fico 
cognitivo Daniel Dennett, que estudia la consciencia y la mente. Un 
capÃ­tulo de su Ãºltimo libro, From Bacteria to Bach and Back ,
 un tratado enciclopÃ©dico sobre la consciencia, sugiere que una parte 
natural de la evoluciÃ³n de la propia inteligencia consiste en 
desarrollar sistemas capaces de ejecutar tareas que sus creadores son 
incapaces de ejecutar. "La pregunta es: Â¿quÃ© adaptaciones 
tenemos que hacer para hacerlo bien, quÃ© estÃ¡ndares debemos exigirles 
que cumplan a ellos, y a nosotros mismos? ", me pregunta dentro de su abarrotado despacho en el idÃ­lico campus de la universidad.  TambiÃ©n me hizo una advertencia sobre la bÃºsqueda del carÃ¡cter 
explicable: "Creo que si vamos a utilizar estas cosas y depender de 
ellas, entonces necesitamos el mejor entendimiento posible de cÃ³mo y por
 quÃ© nos proporcionan respuestas", sugiere. Pero, puesto que podrÃ­a no 
existir ninguna respuesta perfecta, deberÃ­amos mostrarnos igual de 
cautelosos ante las explicaciones de la IA como nos mostramos ante las 
explicaciones humanas, independientemente de lo lista que parezca la 
mÃ¡quina. Dennet concluye: "Si no puede explicar lo que hace mejor que nosotros, entonces no te fÃ­es".            CrÃ©ditos     IlustraciÃ³n: Keith Rankin            Su nombre     Comment *               ComputaciÃ³n  Las mÃ¡quinas cada vez mÃ¡s potentes estÃ¡n acelerando los avances cientÃ­ficos, los negocios y la vida.         CÃ³mo afrontar que internet no ha conseguido crear un mundo mejor  Los
 grandes defensores de la web que confiaban plenamente en su 
capacidad para mejorar la sociedad se han topado con una cruda realidad 
de noticias falsas, odio y falta de privacidad que les ha dividido en 
cuatro grandes grupos de pensamiento: puristas, esperanzados, 
desilusionados y revisionistas  Por Tim Hwang        El Uber de la ciberseguridad une a empresas con cazadores de virus  Ante
 el aumento de ciberataques y la falta de personal cualificado, cada vez
 mÃ¡s compaÃ±Ã­as recurren a estos servicios para que expertos en 
ciberseguridad 'freelance' les ayuden a detectar errores en sus 
cÃ³digos a cambio de recompensas econÃ³micas. Los mejores pueden 
ganar sumas importantes  Por Martin Giles        Crear noticias falsas, al alcance de cualquiera gracias a la IA  Los
 avances en aprendizaje automÃ¡tico han hecho posible el desarrollo de 
herramientas para crear vÃ­deos falsos cada vez mÃ¡s verosÃ­miles. Este 
hecho, sumado a que estas herramientas son cada vez mÃ¡s accesibles 
plantean un futuro en el que el anÃ¡lisis por parte de los expertos serÃ¡ 
fundamental para saber si se trata de realidad o de manipulaciÃ³n  Por Will Knight          MÃ¡s informaciÃ³n sobre ComputaciÃ³n                     SÃ­guenos  Twitter Facebook RSS                             CompaÃ±Ã­a QuiÃ©nes somos  ContÃ¡ctenos   Legal PolÃ­tica de Privacidad  TÃ©rminos y Condiciones     Copyright Â© MIT Technology Review, 2017-2018.                      Usamos cookies en este sitio para mejorar la experiencia de usuario. Al hacer clic en cualquier enlace de esta pÃ¡gina nos da su consentimiento para utilizar cookies.    De acuerdo  MÃ¡s info   